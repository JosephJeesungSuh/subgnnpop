resources:
  cloud: gcp
  accelerators: A100-80GB:2

# Working directory (optional) containing the project directory.
# Its contents are synced to ~/sky_workdir/ on the instance.
workdir: ~/old_projs/subpop

envs:
  # tokens and api keys
  HF_TOKEN: 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
  WANDB_API_KEY: 8f3a80cfc6a25862eb821a30f10ea98fe02afceb

# file mounts
file_mounts:
  /experiment_data:
    source: gs://stpl_model_checkpoints/
    mode: MOUNT

# Typical use: pip install -r requirements.txt
# Invoked under the workdir (i.e., can use its files).
setup: |
  alias ca="conda activate"
  conda create -n subpop python=3.10 -y
  conda activate subpop
  pip install -r requirements.txt
  pip install -e .
  echo "Finished setting up the environement."

# Typical use: make use of resources, such as running training.
# Invoked under the workdir (i.e., can use its files).
run: |
  set -e
  echo "Activating runtime environment..."
  conda activate subpop
  echo "Starting vLLM server in background..."
  torchrun --nnodes=1 \
    --nproc-per-node=2 \
    --master_port=29501 \
    scripts/experiment/run_finetune.py \
    --enable_fsdp \
    --low_cpu_fsdp \
    --fsdp_config.pure_bf16 \
    --use_peft=true \
    --use_fast_kernels \
    --checkpoint_type StateDictType.FULL_STATE_DICT \
    --peft_method='lora' \
    --use_fp16 \
    --mixed_precision \
    --batch_size_training 128 \
    --val_batch_size 256 \
    --gradient_accumulation_steps 1 \
    --dist_checkpoint_root_folder None \
    --dist_checkpoint_folder None \
    --batching_strategy='padding' \
    --dataset_path inductive_individual/opinionqa_0.4 \
    --dataset individual \
    --output_dir /experiment_data/ \
    --name gnn_baseline_inductive_opinionqa_0.4 \
    --model_name meta-llama/Llama-2-7b-hf \
    --model_nickname llama2_7b \
    --lr 2e-4 \
    --num_epochs 10 \
    --weight_decay 0 \
    --loss_function_type ce \
    --which_scheduler cosine \
    --warmup_ratio 0.1 \
    --gamma 0.95 \
    --lora_config.r 8 \
    --lora_config.lora_alpha 32 \
    --is_chat=false \
    --wandb_config.project steerable-pluralism \
    --wandb_config.entity ucb-steerable-pluralism  
